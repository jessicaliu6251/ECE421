import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.python.ops import confusion_matrixfrom tensorflow.python.ops import math_opsimport pandas as pddef loadData():    with np.load('notMNIST.npz') as data :        Data, Target = data ['images'], data['labels']        posClass = 2        negClass = 9        dataIndx = (Target==posClass) + (Target==negClass)        Data = Data[dataIndx]/255.        Target = Target[dataIndx].reshape(-1, 1)        Target[Target==posClass] = 1        Target[Target==negClass] = 0        np.random.seed(421)        randIndx = np.arange(len(Data))        np.random.shuffle(randIndx)        Data, Target = Data[randIndx], Target[randIndx]        trainData, trainTarget = Data[:3500], Target[:3500]        validData, validTarget = Data[3500:3600], Target[3500:3600]        testData, testTarget = Data[3600:], Target[3600:]    return trainData, validData, testData, trainTarget, validTarget, testTargetdef MSE(W, b, x, y, reg):    # Your implementation here    #w=weight matrix, b=bias matrix, x=data matrix, y=label, lambda    N = x.shape[0]    Lw = reg*np.matmul(np.transpose(W), W)/2    #x shape [3500, 784], W shape [784, 1], y shape [3500,1]    temp_mat = np.matmul(x, W)+b-y    #temp_mat shape [3500,1]    Ld = np.matmul(np.transpose(temp_mat), temp_mat)/(2*N)    L = Lw+Ld    return Ldef gradMSE(W, b, x, y, reg):    # Your implementation here    N =x.shape[0]    temp_mat = np.matmul(x, W)+b-y    #temp_mat: 3500x1, x: 3500x784, w_grad=784x1    W_grad = 2*np.matmul(np.transpose(x),temp_mat)/N + reg*W/2    B_grad = 2*np.sum(temp_mat)/N    return W_grad, B_graddef crossEntropyLoss(W, b, x, y, reg):    # Your implementation here    N = len(y)    LW = reg / 2 * np.matmul(W.T, W)    expm = 1/(1+np.exp(-np.matmul(x, W) - b))    LD = - np.matmul(y.T, np.log(expm)) - np.matmul((1-y.T), np.log(1-expm))    LD /= N    return LW + LDdef gradCE(W, b, x, y, reg):    # Your implementation here    N = len(y)    expm = np.exp(-(np.matmul(x, W) + b))    expm = 1 / (1 + expm)    gradient_W = np.matmul((expm - y).T, x).T/N + reg*W    gradient_b = np.sum(expm - y)/N    return gradient_W, gradient_bdef grad_descent(W, b, alpha, iterations, reg, EPS, lossType):    # Your implementation here    loss_train = np.zeros(iterations)    loss_val = np.zeros(iterations)    loss_test = np.zeros(iterations)    acc_train = np.zeros(iterations)    acc_val = np.zeros(iterations)    acc_test = np.zeros(iterations)    if lossType == "CE":        err_train = crossEntropyLoss(W, b, trainData_flatten, trainTarget, reg)        err_val = crossEntropyLoss(W, b, valData_flatten, validTarget, reg)        err_test = crossEntropyLoss(W, b, testData_flatten, testTarget, reg)    else:        err_train = MSE(W, b, trainData_flatten, trainTarget, reg)        err_val = MSE(W, b, valData_flatten, validTarget, reg)        err_test = MSE(W, b, testData_flatten, testTarget, reg)    w_new, b_new = W, b    for i in range(iterations):        if err_train <= EPS:            return w_new, b_new        if lossType == "CE":            w_grad, b_grad = gradCE(w_new, b_new, trainData_flatten, trainTarget, reg)        else:            w_grad, b_grad = gradMSE(w_new, b_new, trainData_flatten, trainTarget, reg)        w_new = w_new - alpha * w_grad        b_new = b_new - alpha * b_grad        if lossType == "CE":            err_train = crossEntropyLoss(w_new, b_new, trainData_flatten, trainTarget, reg)            err_val = crossEntropyLoss(w_new, b_new, valData_flatten, validTarget, reg)            err_test = crossEntropyLoss(w_new, b_new, testData_flatten, testTarget, reg)        else:            err_train = MSE(w_new, b_new, trainData_flatten, trainTarget, reg)            err_val = MSE(w_new, b_new, valData_flatten, validTarget, reg)            err_test = MSE(w_new, b_new, testData_flatten, testTarget, reg)        acc_train[i] = accuracy(w_new, b_new, trainData_flatten, trainTarget)        acc_val[i] = accuracy(w_new, b_new, valData_flatten, validTarget)        acc_test[i] = accuracy(w_new, b_new, testData_flatten, testTarget)        loss_train[i] = err_train        loss_val[i] = err_val        loss_test[i] = err_test        #print(err_train)    return w_new, b_new, loss_train, acc_train, loss_val, acc_val, loss_test, acc_testdef plot_loss_acc(epochs, loss_train, loss_val, loss_test, acc_train, acc_val, acc_test, alpha, reg):    plt.figure(num=1, figsize=(20, 10))    plt.subplot(121)    plt.plot(epochs, loss_train, 'red', label='Training loss')    plt.plot(epochs, loss_val, 'green', label='Validation loss')    plt.plot(epochs, loss_test, 'yellow', label='Test loss')    title = 'Cross Entropy Loss\nalpha = {Alpha}, reg = {Reg}'.format(Alpha=alpha, Reg=reg)    plt.title(title)    plt.xlabel('Epochs')    plt.ylabel('Loss')    plt.legend(['Train', 'Val', 'Test'], loc='upper right')    plt.subplot(122)    plt.plot(epochs, acc_train, 'red', label='Training accuracy')    plt.plot(epochs, acc_val, 'green', label='Validation accuracy')    plt.plot(epochs, acc_test, 'yellow', label='Test accuracy')    title = 'Cross Entropy Accuracy\nalpha = {Alpha}, reg = {Reg}'.format(Alpha=alpha, Reg=reg)    plt.title(title)    plt.xlabel('Epochs')    plt.ylabel('Accuracy')    plt.legend(['Train', 'Val', 'Test'], loc='lower right')    plt.savefig("part2_alpha{Alpha}_reg{Reg}.png".format(Alpha=alpha, Reg=reg))    plt.show()def accuracy(w, b, x, y):    score = np.matmul(x, w) + b    #score = tf.sigmoid(score)    count = 0    for i in range(len(y)):        if score[i] < 0.5:            y_new = 0        elif score[i] >= 0.5:            y_new = 1        if y_new == y[i]:            count += 1    acc = count/x.shape[0]    return accdef flatten():    """    This function flattens the data from [28, 28] to [784, 1]    """    trainData_flatten = np.zeros((trainData.shape[0], trainData.shape[1]*trainData.shape[2]))    for i in range(trainData.shape[0]):        trainData_flatten[i] = np.ndarray.flatten(trainData[i])    valData_flatten = np.zeros((validData.shape[0], validData.shape[1]*validData.shape[2]))    for i in range(validData.shape[0]):        valData_flatten[i] = np.ndarray.flatten(validData[i])    testData_flatten = np.zeros((testData.shape[0], testData.shape[1]*testData.shape[2]))    for i in range(testData.shape[0]):        testData_flatten[i] = np.ndarray.flatten(testData[i])    return trainData_flatten, valData_flatten, testData_flattendef part1():    W = np.zeros((trainData_flatten.shape[1], 1))    b = np.zeros(1)    alpha = 0.005    iterations = 5000    reg = 0.1    EPS = 1e-7    w_train, b_train, loss_train, acc_train, loss_val, acc_val, loss_test, acc_test = \        grad_descent(W, b, alpha, iterations, reg, EPS, "CE")    epochs = range(1, iterations+1)    print(w_train)    print("w_train = {W_train}\n"          "b_train = {B_train}\n"          "loss_train = {Loss_train}\n"          "loss_val = {Loss_val}\n"          "loss_test = {Loss_test}\n"          "acc_train = {acc_train}\n"          "acc_val = {acc_val}\n"          "acc_test = {acc_test}\n".format(w_train, b_train, loss_train[iterations-1],                                           loss_val[iterations-1], loss_test[iterations-1],                                           acc_train[iterations-1], acc_val[iterations-1], acc_train[iterations-1])          )    #alpha=0.005: 1  ; alpha = 0.001:  1  ; alpha = 0.0001: 0.9655172413793104    plot_loss_acc(epochs, loss_train, loss_val, loss_test, acc_train, acc_val, acc_test, alpha, reg)def buildGraph(beta1=None, beta2=None, epsilon=None, lossType=None, learning_rate=None):    training_epochs = 700    mini_batch_size = 500    reg = 0.1    loss_train = np.zeros(training_epochs)    loss_val = np.zeros(training_epochs)    loss_test = np.zeros(training_epochs)    acc_train = np.zeros(training_epochs)    acc_val = np.zeros(training_epochs)    acc_test = np.zeros(training_epochs)    # Your implementation here    tf.set_random_seed(421)    """initialize weights and bias tensors"""    W = tf.get_variable("W", initializer=tf.truncated_normal(shape=(trainData_flatten.shape[1], 1), stddev=0.5, dtype=tf.float64))    b = tf.get_variable("b", initializer=tf.truncated_normal(shape=[1], stddev=0.5, dtype=tf.float64))    """initialize X, Y, and reg tensors"""    X = tf.placeholder(tf.float64, [None, trainData_flatten.shape[1]])    Y = tf.placeholder(tf.float64, [None, 1])    lamda = tf.placeholder(tf.float64)    Z = tf.add(tf.matmul(X, W), b)    y_pred = Z    #implementing MSE and CE loss/predictions separately    if lossType == "MSE":        loss = tf.losses.mean_squared_error(labels=Y, predictions=Z)        y_pred = Z    elif lossType == "CE":        y_pred = tf.sigmoid(Z)        loss = tf.reduce_mean(            tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=Y) + lamda * tf.nn.l2_loss(W))    # this runs one step of gradient descent    adam = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)    optimizer = adam.minimize(loss=loss)    init = tf.global_variables_initializer()    with tf.Session() as sess:        sess.run(init)        for epoch in range(training_epochs):            #shuffling data for each epoch            index_order = np.random.permutation(range(len(trainTarget)))            trainData_shuffled = trainData_flatten[index_order]            trainTarget_shuffled = trainTarget[index_order]            for num_batch in range(int(len(trainTarget)/mini_batch_size)):                _, loss_train_per_batch, w_train, b_train, y_test_hat = \                    sess.run([optimizer, loss, W, b, y_pred],                             feed_dict={X: trainData_shuffled[num_batch * mini_batch_size:(num_batch + 1) * mini_batch_size],                                        Y: trainTarget_shuffled[num_batch * mini_batch_size:(num_batch + 1) * mini_batch_size],                                        lamda: reg})                loss_val_per_batch = sess.run(loss, feed_dict={X: valData_flatten,                                                                      Y: validTarget,                                                                      lamda: reg})                loss_test_per_batch = sess.run(loss, feed_dict={X: testData_flatten,                                                                        Y: testTarget,                                                                        lamda: reg})                print("Epoch:", '%04d' % (epoch + 1), "loss=", "%.9f" % loss_train_per_batch)            #obtaining loss/accuracy values on training, validation and test data for each epoch            loss_train[epoch] = loss_train_per_batch            loss_val[epoch] = loss_val_per_batch            loss_test[epoch] = loss_test_per_batch            acc_train[epoch] = accuracy(w_train, b_train, trainData_flatten, trainTarget)            acc_val[epoch] = accuracy(w_train, b_train, valData_flatten, validTarget)            acc_test[epoch] = accuracy(w_train, b_train, testData_flatten, testTarget)    #plot curves    epochs = range(1, training_epochs+1)    plot_loss_acc(epochs, loss_train, loss_val, loss_test, acc_train, acc_val, acc_test, learning_rate, reg)    returntrainData, validData, testData, trainTarget, validTarget, testTarget = loadData()trainData_flatten, valData_flatten, testData_flatten = flatten()#part1()#part3buildGraph(beta1=0., beta2=0.95, epsilon=1e-7, lossType="MSE", learning_rate=0.001)